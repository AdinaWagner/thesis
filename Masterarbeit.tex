% Autor: Adina Wagner, 215486

%---------- Pakete und Dokumenteinstellungen -------------------------------------

\documentclass[a4paper, 12pt]{scrreprt}

\usepackage[ngerman, american]{babel}   %andersherum, also "ngerman, american" sind die Ueberschriften in Englisch
\usepackage[T1]{fontenc}			%Kodierung des Zeichensatzes
\usepackage[utf8]{inputenc}		%Dt. Umlaute mit Schriftsatz
%\usepackage[latin1]{inputenc}
\usepackage[a4paper, left=2.5cm, right=2.5cm, top=3cm, bottom=3cm]{geometry}
\usepackage{latexsym}
\usepackage{amsmath}		% Mathe-Paket
\usepackage{amsthm}
\usepackage{graphicx}		% Paket für Graphiken
\usepackage{color,psfrag}
\usepackage{amssymb}		% spez. Mathe-Symbole
\usepackage{dsfont}
\usepackage{framed, color}
\usepackage[automark,headsepline]{scrpage2}
\usepackage{eurosym}		% Euro-Zeichen Symbol via \euro, bzw. \EUR{x,yz} liefert x,yz €
\usepackage{leftidx}
\usepackage{longtable}
\usepackage{array}
\usepackage{stmaryrd}		%Widerspruchsblitz via \lightning
\usepackage{enumitem} %Anpassbare Enumerates/Itemizes
\usepackage{trfsigns} % \e, \im
\usepackage{dlfltxbcodetips }	%\bigtimes
\usepackage{rotating} 	% Tabellen im Querformat
\usepackage{floatrow} % Caption neben Abbildungen
% \usepackage{makecell} % dicke Linien in Tabellen
%\usepackage{setspace}

\usepackage[toc,page]{appendix}	% Anhang

\usepackage[linesnumbered, ruled]{algorithm2e}	% Algorithmus
\usepackage{listings}	% Code einbinden, aufrufen mit \lstinputlisting{source_filename.py}


\automark[section]{chapter}	%setzt Seitenüberschriften

\geometry{	left=30mm,			%innerer Seitenrand
			top=25mm,				%Seitenoberkante
			width=155mm,			%Textbreite
			height=247mm,			%Texthöhe
			marginparsep=5mm,		%Abstand Notizrand
			marginparwidth=20mm		%Breite Notizrand
		}
		
%------------Abbildungen-----------------------------------------------

\usepackage{graphicx}					%Einbinden von Bildern
\usepackage{calc}						%Berechnen von Längen
\usepackage{float}						%Abb.& Tabellen exakt einbinden mit [H]-Zusatz
\usepackage{subfigure}					%Subfigures mehrere Bilder nebeneinander

\usepackage[format=hang, justification=justified]{caption}	%Bildunterschrift

\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}} %zentrierte Spalten mit fester Breite

% Spezialpakete für tikzpicture
\usepackage{fp}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{pgfplots}

% TikZ-Bibliotheken
\usetikzlibrary{arrows}
\usetikzlibrary{shapes}
\usetikzlibrary{decorations.pathmorphing}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{decorations.shapes}
\usetikzlibrary{decorations.text}


%\usepackage{scrpage2}
\pagestyle{scrheadings}			

%\clearscrheadings	
%\clearscrplain		
\clearscrheadfoot
\ihead[]{\leftmark}						%setzt Chapter-Name als linke Seitenüberschrift
\ohead[]{***DRAFT*** v.0.1 DEC 2018} 				%für sectionname \rightmark einsetzen
%\cfoot[\pagemark]{\pagemark}
\cfoot[\hfill -- \thepage{} -- \hfill]{\hfill -- \thepage{} -- \hfill}	%setzt Seitenzahl unten
\setkomafont{pagefoot}{%
\normalfont\sffamily}

\linespread{1.1} 						%Zeilenabstand
\setlength{\parindent}{0cm} 	%keine Einzüge

\setlength{\unitlength}{3ex}		% Längengrundeinheit auf 3-fache Höhe von "x" setzen.

% dicke Linien in Tabellen: \thickhline 

\makeatletter
\def\thickhline{%
	\noalign{\ifnum0=`}\fi\hrule \@height \thickarrayrulewidth \futurelet
	\reserved@a\@xthickhline}
\def\@xthickhline{\ifx\reserved@a\thickhline
	\vskip\doublerulesep
	\vskip-\thickarrayrulewidth
	\fi
	\ifnum0=`{\fi}}
\makeatother

\newlength{\thickarrayrulewidth}
\setlength{\thickarrayrulewidth}{4\arrayrulewidth} % hier Dicke einstellen


%------------ Eigene Definitionen und Befehle -----------------------------

\newtheorem{Theorem}{Theorem}[chapter]
\newtheorem{Lemma}[Theorem]{Lemma}
\newtheorem{Cor}[Theorem]{Corollary}
\newtheorem{Prop}[Theorem]{Proposition}
\newtheorem{Code}[Theorem]{Code}
\newtheorem{Assumption}[Theorem]{Assumption}
\newtheorem{Construction}[Theorem]{Construction}
\newtheorem{Motivation}[Theorem]{Motivation}
\newtheorem{Def}[Theorem]{Definition}
\newtheorem{Remark}[Theorem]{Remark}
\newtheorem{Ex}[Theorem]{Example}
\def\ci{\perp\!\!\!\perp} %stochastisch unabhängig
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Rquer}{\overline\RR} 
\newcommand{\Null}{{\mathrm{Null}}}
\newcommand{\Range}{{\mathrm{Range}}}
\newcommand{\trace}{{\mathrm{trace}}}
\newcommand{\diag}{{\mathrm{diag}}}
\newcommand{\rank}{{\mathrm{rank}}}
\newcommand{\mm}{{\mathrm{m}}}
\newcommand{\supp}{{\mathrm{supp}}}
\newcommand{\logit}{\mathrm{logit}}
\newcommand{\odds}{\mathrm{odds}}
\newcommand{\qede}{\qquad \hfill \fbox{}}

\newcommand{\ind}{\mathbb{1}_}
\newcommand{\ew}{\mbox{\textbf{E}}}
\newcommand{\var}{\mbox{\textbf{Var}}}
\newcommand{\cov}{\mbox{\textbf{Cov}}}
\newcommand{\cor}{\mbox{\textbf{Corr}}}
\newcommand{\FF}{\mathfrak{F}}
\newcommand{\NN}{\mathbb{N}_0}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\TT}{\mathfrak{T}}
\newcommand{\wra}{$(\Omega,\mathcal{F},\PP)$ }
\newcommand{\mc}{\multicolumn{2}{l}}
\newcommand{\nn}{\nonumber}
\newcommand{\bs}{\begin{upshape}}
\newcommand{\es}{\end{upshape}}
\newcommand{\norm}{\|}

\newcommand*{\discup}{\cup \hspace{-1.6ex} \cdot \hspace{0.6ex}}


\renewcommand{\labelitemii}{$\bullet$}
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tablename}{\normalsize Table:}
\renewcommand{\im}{\mathrm{i}}
%\setheadsepline{0.4pt}
\setcounter{tocdepth}{2}

%------------Literaturverzeichnis--------

\usepackage[babel,german=quotes]{csquotes}
\usepackage[	style=authoryear,		% oder numeric
							backend=bibtex,			% oder biber
							firstinits=true,		% Vorname abgekürzt
							maxitems= 6				% maximale Anzahl an Authoren, Abkürzung et alter
						]{biblatex}
\renewbibmacro{in:}{} 	% unterdrücke "in:" vor dem Journal-Titel						
%\renewcommand*{\mkbibnamelast}[1]{\textsc{#1}} 	% Autoren in Kapitälchen
\setlength{\bibhang}{2em}		% hängender Einzug
						
\addbibresource{Literatur.bib}			% Pfad zur Datei im selben Ordner




% --------- Abkürzungsverzeichnis ------------------
\usepackage{nomencl}	% Package für Abkürzungsverzeichnis
\setlength{\nomitemsep}{-\parsep} % Zeilenabstand 
\setlength{\nomlabelwidth}{.15\hsize}	% Erklärungen bündig
\makenomenclature
%%% Ausführen in der Konsole

% D:
% cd STUDIUM\_Master\Masterabeit\LaTeX\LaTeX aktuell
% makeindex Masterarbeit.nlo -s nomencl.ist -o Masterarbeit.nls
% nice, das funktioniert unter Debian genauso


% ---------------- Dokument ----------------------

\begin{document}

% ------------ Deckblatt --------------------------

\begin{figure}[h]
\vspace{-1.5cm}
\hspace{9.5cm}
\includegraphics[scale=0.5]{img/ovgu_nat_logo}
\label{logoOVGU}
\end{figure}

\begin{center}
\bigskip
\begin{LARGE}
\textbf{Masters' Thesis}
\end{LARGE}

\vspace{\fill}

\begin{huge}
%\begin{scshape}
 
\textbf{Catching the eye}: \\
Investigating the functional neuroanatomy of the visuospatial attention system with fMRI and eyegaze recordings during natural stimulation

%\end{scshape}
\end{huge}

\vspace{\fill}

submitted by\\
\begin{large}
Adina Wagner (215486) \\
\vspace{0.3cm}
Master of Science in Clinical Neuroscience\\
\begin{normalsize}
adina.wagner@t-online.de 
\end{normalsize}

\vspace{\fill}

\begin{normalsize}
submitted to\\
\end{normalsize}
J.-Prof. Dr. Michael Hanke\\
Prof. Dr. Stefan Pollmann\\
\vspace{0.5cm}
Psychoinformatics Lab\\
Otto-von-Guericke Universität Magdeburg\\
\end{large}

\vspace{1cm}

February $3^{rd}$, 2019

\thispagestyle{empty}
\end{center}
\clearpage


% -------- Ende Deckblatt ----------------------

% ------- Leere Seite nach dem Deckblatt -------
%\newpage 
%\thispagestyle{empty}
%$\phantom{.}$
%\clearpage


%---- Eigenständigkeitserklärung -----------------


\chapter*{Statutory Declaration}
\addcontentsline{toc}{section}{Statutory Declaration}

I declare that I have authored this thesis independently, that I have not used
other than the declared sources/resources, and that I have explicitly marked
all material which has been quoted either literally or by content from the
used sources.

\bigskip

\begin{center}
	***
\end{center}

\bigskip

Hiermit versichere ich, dass ich die vorliegende Arbeit selbständig und nur
unter Benutzung der angegebenen Literatur- und Hilfsmittel angefertigt
habe. Wörtlich übernommene Sätze und Satzteile aus anderen Druckwerken
oder aus Internetpublikationen sind als Zitat belegt, andere Anlehnungen
hinsichtlich Aussage und Umfang unter Angabe der Quelle kenntlich gemacht.
Die Arbeit wurde in gleicher oder ähnlicher Form in keiner anderen
Lehrveranstaltung als Leistungsnachweis eingereicht.
Ich bin darüber unterrichtet, dass die Lehrenden angewiesen sind, schriftliche
 Arbeiten zu überprüfen, und dass ein Vergehen eine Meldung beim
Prüfungsausschuss der Fakultät zur Folge hat, die im schlimmsten Fall zum
Ausschluss aus der Universität führen kann.

\bigskip

\begin{center}
	***
\end{center}

\bigskip

Magdeburg, February $3^{rd}$, 2019

\bigskip

\bigskip

\bigskip

---------------------------------------------------

$\phantom{mmmm..}$  (Adina Wagner)

\clearpage


% ------- Schlauer Spruch --------------------
\chapter*{ }

\renewenvironment{quote}
	{\list{}{\rightmargin=1cm \leftmargin=5cm} %
		\item \relax}
	{\endlist}

\begin{quote}
\textit{The cure for boredom is curiosity. There is no cure for curiosity.}

\medskip
-- Ellen Parr 
\end{quote}


\clearpage

% ------- Inhaltsverzeichnis ------------------
\addcontentsline{toc}{section}{Table of Contents}
\tableofcontents

% ---------- Abbildungsverzeichnis -------------
\clearpage
\addcontentsline{toc}{section}{List of Figures}
\listoffigures
\clearpage

\addcontentsline{toc}{section}{List of Tables}
\listoftables
\clearpage

\addcontentsline{toc}{section}{List of Algorithms}
\listofalgorithms
\clearpage

% ---------- Abkürzungsverzeichnis -------------
\clearpage
\addcontentsline{toc}{section}{List of Abbreviations}
\printnomenclature

\nomenclature{FFA}{Fusiform Face Area}
\nomenclature{EBA}{Extrastriate Body Area}
\nomenclature{PPA}{Parahippocampal Place Area}
\nomenclature{LOC}{Lateral Occipital Complex}
\nomenclature{fMRI}{functional Magnetic Resonance Imaging}
\nomenclature{PET}{Positron Emission Tomography}
\nomenclature{GNB}{Gaussian Naive Bayes [classifier]}
%\nomenclature{SVM}{Support Vector Machine}
\nomenclature{FEF}{Frontal Eye Field}
\nomenclature{TPJ}{Tempoparietal Junction}
\nomenclature{STS}{superior temporal sulcus}
\nomenclature{STG}{superior temporal gyrus}
\nomenclature{MFG}{Medial frontal gyrus}
\nomenclature{IFG}{Inferior frontal gyrus}
\nomenclature{IPS}{Intraparietal Sulcus}
\nomenclature{MF}{Magnification Factor}
\nomenclature{SEF}{Supplementary Eye Fields}
\nomenclature{TMS}{Transcranial magnetic stimulation}
\nomenclature{MEG}{Magneto-encephalography}
\nomenclature{GLM}{General linear model}
\nomenclature{OLS}{ordinary least-squares}
\nomenclature{SSE}{explained sum of squares}
\nomenclature{SSR}{residual sum of squares}
\nomenclature{SST}{total sum of squares}
\nomenclature{MSE}{mean squared error}
\nomenclature{rTMS}{repetetive transcranial magnetic stimulation}
\nomenclature{}{}
\nomenclature{}{}



\clearpage


%---------- Acknowledgments -------------

\chapter*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

I am thankful to a large number of people who supported and challenged me on the path of my master thesis completion and beyond. First and most important of all, I would like to thank my scientific supervisor J.-Prof. Dr. Michael Hanke of the Psychoinformatics Lab. He introduced me to the mesmerizing field of Psychoinformatics and has provided me with more opportunities I can recall to take root in it. A list of things that would not have been possible without him would span pages. I attribute much of my personal growth and scientific advancement of the past 1.5 years to him, and I am very grateful to continue pursuing my academic career as a PhD student under his supervision. \newline
Secondly, I am grateful to Prof. Dr. Pollmann for agreeing to be the second assessor of my Masters Thesis, and for valuable discussions and insights with regard to the thesis.\newline
I cannot overstate the importance of Prof. Yaroslav Halchenko, whom I want to thank deeply for his agreement to supervise and mentor me on a four month research stay at Jim Haxbys Lab at Dartmouth College, NH, USA. His and Michaels passion for open science, open-source software, and scientific integrity and reproducibility will continue to be my source of inspiration and motivation throughout my scientific career.\newline
I further am grateful to Prof. Jim Haxby for welcoming me in his lab in Dartmouth, as well as Prof. Ida Gobbini, Vassiki Chauhan, Kyle Meyer and Feilong Ma for their company and advice throughout my stay in the US. \newline
This Masters Thesis would not have been possible without the methodological discussions, programming tutorials, and most importantly friendship of Alexander Waite, Dr. Emanuele Porcu, Benjamin Poldrack, Christian Häusler and Asim Dar. All members of the Psychoinformatics Lab have contributed to the most welcoming, fun, and empowering work environment I have ever been a part of, and despite my daily commute, this wonderful lab became a family for me. \newline
All of my studies, in Germany and abroad, were mainly funded by the German Academic Foundation. The ideal and financial support since 2013 enabled me to become the person I am today. I am thankful to all of my referees, Dr. Ludwig, Dr. Trebesius, Dr. Köhne, and Dr. Chwalleck, for the insightful discussions, and my mentors, Prof. Enders and Prof. Fink, for their assistance and guidance, and to Dr. Julius and Prof. Zimmermann, whom I was honored to meet and work with several times. \newline
Last but not least I am heartily grateful to my parents and friends for their patience, their support, their steady encouragement and constant moral guidance. Especially Gunnar Behrens, who was my anchor throughout seven busy years.


\clearpage

\pagenumbering{arabic}
\setcounter{page}{1}	% Beginn der Textseitenzaehlung

%---------- Eigentliche Ausarbeitung ----------

\chapter*{TODO: Abstract}
\addcontentsline{toc}{chapter}{Abstract}

[TODO] \newline
[Introduce the topic] \newline
[short statement of relevance - what is a gap in the literature]  \newline
[what is the aim of the thesis] \newline
The analysis attempts to shed light on the functional specificity of one particular part of the visuospatial attention system, the frontal eye field [FEF]. For this aim, a multi-step procedure was employed. In a first step, a novel method to derive functional specificities of regions of interests was implemented and validated on data from a standard localization paradigm and a movie watching paradigm of the studyforrest dataset. In parallel, eye gaze data of participants were utilized both to derive a measure of visuospatial attention, and saccadic eye movements. 
[We're still missing the main approach here...]
[what was the main result] \newline
[if there's any space left, conclusions.]

\bigskip

\textbf{Keywords:} \textit{fMRI, eye-tracking, naturalistic stimulation, methods, localization}



\chapter{Introduction}\label{section:intro}

In complex natural environments, attention is necessary to handle an excess of information despite limited neural computing power (\cite{carrasco2011visual}). It serves as a filter to select potential behaviorally or cognitively relevant cues from the continuous stream of information, and as a gatekeeper to discard irrelevant stimuli in order to prevent sensory overload (\cite{bellebaum2012neuropsych}). \textit{Visual attention} provides this filtering function for visual perception: Relevant visual stimuli are prioritized and hence attended while less important aspects of a scene are neglected. In this way, visual attention is crucial for the selection of cognitive and behavioral strategies for interacting with the environment (\cite{siegelbaum2000neuro}).\newline
One integral functional region involved in both the selection and attendance of behaviorally relevant aspects of the visual environment is the \textit{frontal eye fields} (FEF). They serve core functions in the movement of the eyes (\cite{krauzlis2014eye}), and within the dorsal endogenous attention system (\cite{corbetta2002control}; \cite{corbetta2008reorienting}). While well localized and understood in animal models via electric stimulation (e.g. \cite{bruce1985primate}), non-invasive research into the location and function of human FEF under natural conditions is more challenging. Current literature displays a remarkable diversity of locations between subjects and methods (see \ref{section:shortcomings}, \cite{vernet2014corrigendum}), and highly controlled experimental conditions lacking the ecological validity of natural stimulation may fail to evoke and study the attentional modulation of the FEF. \newline
This thesis aims to enrich current literature on the functional neuroanatomy and location of the FEF by means of methodological advancements in two different aspects: For one, it attempts to utilize information derived from eye-tracking data for a measure visuospatial attention deployment. Secondly, it introduces a novel approach for determining the specificity of functional regions of interest. Lastly, it attempts to combine both aspects to gain insights into the location and attentional modulation of the FEF during naturalistic stimulation with the Hollywood movie Forrest Gump. \newline
This thesis is structured as follows. Chapter \ref{section:intro} gives an overview of visuospatial attention and its underlying neuroanatomical foundation with an additional focus on the frontal eye fields (section \ref{section:visualattention}), their relation to the movements of the eye (section \ref{section:eyemoves}), and the utilization of eye movements in the study of attentional deployment (section \ref{section:eyeutils}). Furthermore, it outlines current localization paradigms with their potential shortcomings, and describes a novel method to derive functional information from an ROI localization (section \ref{section:methodintroduction}). Chapter \ref{section:methods} introduces the data basis. It elaborates on the chosen method for scanpath comparison, \textit{multimatch}, and the developed ROI specificity analysis and its validation. Chapter \ref{section:results} presents the results of the analyses, and chapter \ref{section:discussion} discusses their implications and limitations. \textbf{TODO: Extend this introduction to include more about the motivation and aims -- this is too short.}\pagebreak


\section{Visuospatial attention and the frontal eye fields}\label{section:visualattention}
The exogenous (greek \textit{exo} = outside, \textit{genein} = to produce) visual attention, also referred to as involuntary or bottom-up controlled, visual attention, is driven by the context-dependent saliency of external stimuli (\cite{itti2001computational}). If the physical visual properties like color, luminance, motion or contrasts of a stimulus are are conspicuous enough given the context they are in, they are able to capture the visual attention even if the observer doesn’t intent to orient attention to it (\cite{chica2013two}). According to \textcite{corbetta2002control} attention framework, a ventral attention network consisting of the tempoparietal junction (TPJ)/superior temporal sulcus and gyrus (STS, STG), the medial/inferior frontal gyri (MFG/IFG), and the ventral part of the supramarginal gyrus (SMG) processes attentional selection based on such exogenous cues (Figure \ref{fig:Networks}, orange areas), if they are behaviorally relevant (\cite{downar2000multimodal}). While there is some differential evidence for the TPJ (see \textcite{vossel2014dorsal}, for a short overview) a majority of studies showed a lateralization of the ventral system to the right hemisphere (\cite{corbetta2002control}; \cite{fox2006spontaneous}; \cite{corbetta2008reorienting}). \newline 
The endogenous (greek \textit{endo} = within) visual attention, also termed voluntary or top-down controlled visual attention, on the other hand is controlled by selection criteria that depend on cognitive factors such as requirements of the current task, prior experiences or expectations (\cite{itti2001computational}). This goal-directed selection is controlled by a dorsal frontoparietal network including the intraparietal sulcus (IPS) and superior parietal lobule (SPL), and the dorsal frontal cortex along the precentral sulcus, near or at the fronal eye field (FEF) and the supplementary eye field of each hemisphere (SEF; figure \ref{fig:Networks}, blue areas; \cite{corbetta2002control}; \cite{corbetta2008reorienting}). Endogenous attention is usually drawn to only weakly salient stimuli that would not evoke exogenous attention but are relevant for current cognitive aims. As such, whereas exogenous attention is a rapid, automatic orientating response, endogenous attention is a voluntary and controlled process of information monitoring (\cite{carrasco2011visual}).

\begin{figure}
	\includegraphics[scale=0.4]{img/attentionnetworks.png}
	\caption[Dorsal and ventral attention networks]
	{\small{Definition of dorsal and ventral networks from activation data and putative interactions. Taken from \textcite{corbetta2008reorienting}.}}
	\label{fig:Networks}
\end{figure}

A core part of the dorsal attention system is the FEF. In humans, the FEF is located bilaterally in the rostral bank of a portion of the precentral sulcus at the caudal end of the middle frontal gyrus. Early insights about its function stem from stimulation studies with primates and were related to the FEF functions in the generation of eye-movements, in particular saccades: Low-intensity electrical stimulation of the FEF in the bank of the arcuate sulcus in monkeys elicits saccadic eye movements, directed contralaterally to the stimulated hemisphere (\cite{tehovnik2000eye}). Work by \textcite{bruce1985primate} further found that saccades elicited by stimulation at a particular region have a particular direction and amplitude that is independent of the orientation of the eyes, thus finding the first evidence of a subdivision within the FEF. Their work revealed a gradient of saccadic amplitudes from lateral to medial sites of the FEF: The more medially the stimulation of the primate arcuate sulcus, the larger the saccadic amplitude (\cite{bruce1985primate}). Hence, the ventrolateral portion of the FEF is generating shorter saccades, while the mediodorsal portion is generating longer saccades.  Extensive connectivity between the FEF and the visual cortex or other areas related to vision such as the dlPFC provide evidence for a substantial role of the FEF in vision (\cite{stanton1995topography}). Even the aforementioned partialization of the FEF is propagated within connections with other visual areas caudal to the central sulcus. The more ventrolateral portion of the FEF, responsible for generating shorter saccades, is interconnected with retinotopically organized areas that represent central vision (i.e. within ~10$^\circ$ of the fovea) in the inferotemporal cortex. In contrast, the mediodorsal FEF, responsible for generating longer saccades, is interconnected with areas with a representation of the peripheral visual field (\cite{stanton1995topography}, \cite{schall1995topography}).
Stimulation studies in primates and high resolution fMRI experiments in humans further revealed that the FEF also contains zones that are involved in the  control of other types of eye movements (see \textcite{krauzlis2014eye} for an overview): Stimulation of the frontal pursuitzone in the fundus elicits pursuit eye movements directed ipsilateral to the stimulated hemisphere (\cite{blanke2003direction}). The intracortical stimulation of several subareas within the FEF, among others a region within the pre-arcuate cortex in rhesus monkeys, immediately rostral to the saccade related anterior bank of the arcuate sulcus, triggers vergence movements (changes of the depth of the gaze), and shows an involvement in accomodation (Crosby et al., 1952, cited by \textcite{vernet2014corrigendum}). The FEF is further active in the disengagement from fixations prior to a new saccade (\cite{goodwin2007cranial}; \cite{tehovnik2000eye}). It therefore plays a crucial motor function in most types of eye movements, most prominently saccades, and also pursuit, fixation, and vergence eye movements. \newline
However, as its involvement in the dorsal attention network suggests, the FEF is further involved in a number of aspects of higher cognition, such as attention. The most compelling evidence stems from lesion studies: unilateral damage to the FEF results in contralateral neglect (\cite{crowne1981effects}). In healthy humans, \textcite{muggleton2003human} found that repetitive transcranial
magnetic stimulation (rTMS) at 10Hz for 500ms over the right FEF impairs only subtypes of visual search involving endogenous attention, namely where a visual target is neither salient nor predictable. Further studies of both attentional and motor processes of the FEF found that the FEFs functions in motor control and attention can be partially dissociated: activation in the FEF reflects deployment of attention even in the absence of accompanying eye movements to the region of interest, so called \textit{covert attention} (\cite{vossel2014dorsal}), and medical inactivation of the FEF in monkeys disrupts covert visual search tasks (\cite{monosov2009frontal}). Nevertheless, attentional processes and motor functions of the FEF seem closely related. \textcite{thompson2005visual} studied the FEFs role in saccade target selection in primates performing a visual search task. They found evidence for the existence of a heatmap-like representation of saliency encoded in selective activation that is related to the overall behavioral relevance of visual stimuli. This representation, the authors argue, arising from an integration of both top-down and bottom-up influences, then determines targets for foveation in the visual field in a 'the-winner-takes-it-all' fashion. \newline 
The frontal eye fields are therefore an interesting structure occupying important roles in aspects of higher cognition, in particular visuospatial attention, eye movements, and their interaction: Following \textcite{vernet2014corrigendum} reasoning and the short overview given in this section, studies of the frontal eye field in conjunction with the cognitive context of the viewer could shed light on how it modulates the activation of the FEF and its involvement in the various types of eye movements, and gain insight on its role as a site of conversion for the selection of visual targets for eye movements and visuospatial attention (\cite{corbetta2002control}; \cite{kowler2011eye}). \newline
\textbf{TODO: THIS NEEDS A BETTER OUTRO.}


\section{Movements of the eye}\label{section:eyemoves}
The close interaction between attention and eye movements is further eminent in the underlying neuroanatomical principles of human vision. A central feature of the human eye is the fovea centralis, a specialised region about 1.5mm in diameter in the center of the retina (\cite{benninghof2004anat}). It serves only the central 1$^\circ$ of the visual field, but due to possessing the highest amount of cones of the retina, and an asymmetric distribution of ganglion cell density across the retina that advantages foveal information, it provides the greatest visual acuity (\cite{perry1986ganglion}). This asymmetry is propagated in the cortical representation of visual inputs from the retina. The magnification factor (MF), the linear extent of cortex devoted to each linear degree on the retina, increases monotonically from periphal to foveal vision (\cite{daniel1961representation}). As a result, in almost all visual brain areas, both cortically and subcortically, the fovea has the greatest representation, and full visual acuity can therefore only occur at the fovea. 
Due to this constraint, humans need the ability to, first, align the fovea rapidly to an object of interest in a visual scene and, second, keep the fovea aligned to it for a sufficient amount of time in order to maximize the efficiency of foveal vision and perceive objects of interest in greatest detail. An exploration of a visual scene is hence performed in a stepwise manner and requires the described sequence of eye movements to sequentially explore all areas of interest. To accomplish this, the human eye is capable of a number of different eye movements. In its most simple form, the sequence consists of rapid \textit{saccadic} eye-movements, redirecting the fovea from one object of interest to the next, followed by phases of \textit{fixation} that keep the fovea aligned for visual analysis. As saccades reach speeds of up to 500 $^\circ$/sec, small, corrective post-saccadic oscillations, so called \textit{glissades}, correct minor deviations between saccade target and actual saccade endpoint. Additionally, \textit{smooth pursuits} enable a viewer to track moving objects, while \textit{vergence} movements are necessary to perceive different depths (\cite{holmqvist2011eye}). The following table \ref{table:eyemovements} provides an overview of these movements and their characteristics. Note, however, that most work on eye movements was conducted on static stimuli, and eye movements on moving images could possess differential characteristics. \textcite{dorr2010variability} for example investigated fixation characteristics differentially for varying stimuli and found that Hollywood movies elicit longer fixations (mean: 354ms, median: 253ms) than static images. \newline
  
\begin{table}[H]
	\begin{center}
		\begin{tabular}{lC{3cm}C{3cm}C{3cm}}
			\thickhline 
			\textbf{Type}	& \textbf{Duration(ms)} & \textbf{Amplitude (degrees)} & \textbf{Velocity ($^\circ$/sec)} \\ \hline
			Fixation 		& 200-300  & -  &  - \\
			Saccade 		& 30-80   & 4-20  & 30-500 \\
			Glissade		& 10-40   & 0.5-2  & 20-140 \\
			Smooth pursuit	& -  & -  &  10-30 \\ \thickhline
		\end{tabular}
		\caption[Movements of the eye]{\small{An overview of major types of eye movements and their characteristics, taken from \textcite{holmqvist2011eye}.}}
		\label{table:eyemovements}
	\end{center}
\end{table}

With regard to the scope of this thesis, and as eminent from the fundamental principles of foveal vision, saccades are the type of eye movement of particular relevance to the study of visuospatial attention (\cite{kowler2011eye}). As mentioned in the previous section, the FEF, core part of the dorsal attention network, is playing a key role in it. Saccadic eye movements are controlled by a number of highly interconnected brain regions, both cortical and subcortical (\cite{munoz2002commentary}, see figure \ref{fig:saccregions}), but in the cortex, the FEF are seen as the principal region involved in their oculomotor control (\cite{leichnetz1988higher}). Thus, in regard to the study of the functional neuroanatomy of the visuopatial attention system by means of eye movement, saccades appear as a plausible choice of target movements.


\begin{figure}[H]
	\includegraphics[scale=0.5]{img/saccregions.png}
	\caption[Regions involved in saccadic eye movements.]{\small{Brain areas involved in the control of saccadic eye movements in monkey (left) and human (right). \textit{Cb}: Cerebellum, CN: Caudate nucleus, \textit{DLPFC}: dorsolateral prefrontal cortex, \textit{FEF}: Frontal eye field, \textit{LIP}: Lateral intraparietal area; \textit{RF}: Reticular formation, \textit{SC}: superior colliculus, \textit{SEF}: supplementary eye field, \textit{Snr}: substantia nigra pars reticularis, \textit{Th}: Thalamus. (Taken from \textcite{munoz2002commentary})}\textbf{Todo: this might be superflous, kick out if in need of space}}
	\label{fig:saccregions}
\end{figure}

\section{Insights on function and location from naturalistic stimulation}\label{section:shortcomings}
Most research paradigms in neuroscience employ highly controlled stimulation protocols to precisely pinpoint and disentangle the function or location of the structure in question (\cite{malinen2007towards}). This approach however can suffer from a number of shortcomings in the study of attention or in the localization and study of functional regions of interest. The following section outlines a number of these shortcomings and argues for the use of naturalistic stimulation for additional insights instead. \newline 
Visuospatial attention is usually studied by means of highly controlled experimental tasks, such as Posners' cueing paradigm (\cite{posner1980attention}) or simple visual search tasks. While well controllable, these conventional laboratory experiments suffer from a lack of ecological validity and fail to evoke realistic competing demands for visuospatial attention. As \textcite{hasson2004intersubject} noted, controlled experimental settings bear little resemblance to natural viewing for at least four reasons: 1) Lack of complex visual scenes (i.e. presentation of visual stimuli in isolation), 2) Lack of (complex) movement, 3) Lack of unconstrained eye movement, and 4) Lack of interactions between vision and additional modalities, context and emotional valence. Naturalistic stimulation such as watching movie clips, in contrast, does not suffer from these shortcomings. Therefore, while static images have been an object of interest in the analysis of gaze distribution and attention from the early works of \textcite{yarbus1967eye} onwards, during the past decade, movies or videos became a promising stimulus choice: They contain interesting, multisensory stimuli and are more representative of natural vision arrays than static pictures. As movie content constantly changes and includes both salient elements such as motion, as well as top-down components from the attempts to comprehend and interpret the content (\cite{ross2013eye}), movies represent an ideal possibility to study the dynamic and complex interplay of attentional processes ecologically valid (e.g. in \textcite{hasson2004intersubject}; \cite{carmi2006visual}; \cite{tseng2009quantifying}; \cite{dorr2010variability}). \newline
Localization of ROIs is most commonly achieved by simple contrasts in a general linear model (GLM) between experimental conditions that are known to activate functional regions. To localize the fusiform face area (FFA) for example, a standard paradigm consists of the contrast in activation during block stimulation with images of faces, and images from a different category such as houses (\cite{sengupta2016studyforrest}, \cite{fox2009defining}).
Localization of areas involved in motor tasks such as the FEF would most reliably be conducted via micro-stimulation, which is rarely possible in human subjects. Localization through non-invasive methods is more challenging, and usually employs simple behavioral paradigms with either general oculomotor tasks (\cite{paus1996location}), or more specific oculomotor tasks such as instructed fixations, pro- and antisaccades (\cite{connolly2002human}), or tracking of horizontal step stimuli (\cite{alkan2011differentiation}). However, while capable of localizing areas undoubtedly involved in the movement of the eyes, the displayed movements are not performed under unconstrained conditions. For regions involved in higher cognition such as the FEF it remains unknown whether uncontrolled eye movements under naturalistic viewing behavior can be capable of providing similar, if not even additional information when taking cognitive context such as attention deployment into account. Previous localization studies employing positron emission tomography (PET) (e.g. \textcite{paus1996location}, \cite{kawashima1998oculomotor}), magneto-encephalography (MEG) (e.g. \cite{ioannides2004meg}), fMRI (e.g. \cite{petit1999functional}; \cite{connolly2002human}), and transcranial magnetic stimulation (TMS) (see \textcite{vernet2014corrigendum}, for an overview) were able to shed light on the possible location of the FEF in humans. However, an overall view of this literature also reveals the large variability of reported localizations between studies (see e.g. \textcite{paus1996location}; \textcite{vernet2014corrigendum}, for overviews), and there is no consensus on whether these discrepancies arise from methodological differences in the choice of imaging technique or behavioral paradigm, or inter-individual differences between participants. A more naturalistic paradigm might be able to provide additional answers to this question. \newline
To address the blank spaces left by paradigms with highly controlled, simplistic stimuli or tasks, the present thesis aims to use a more ecologically valid stimulation paradigm by using fMRI and eye-tracking data obtained during movie watching. Due to its ready availability, data richness, and employment of naturalistic stimuli in form of the Hollywood movie "Forrest Gump", the studyforrest datasets were selected as the data basis of this thesis. \textbf{TODO: this outro just reads super stiffly}

\section{Utilizing eye movements in the study of attentional deployment}\label{section:eyeutils}

In order to include the aspect of visuospatial attention, a quantitative measure of attentional deployment needs to be derived. One possibility to do this is by using eye gaze information in the form of \textit{scanpaths}.
The term scanpath refers to the trace of eye-movements in space and time (\cite{holmqvist2011eye}). In its most simple form, it is formed by a succession of fixations and saccades that define the particular sequence in which the eyes explore a visual scene (\cite{anderson2015comparison}). As opposed to other measures that summarize eye gaze, such as heatmaps, the order of eye-movements is relevant - a different order of elements in the representational sequence of eye-movements constitutes a different scanpath. \newline
The analysis of scanpaths has been used for decades to gain insights into the viewers’ mental processes, especially those concerning visuo-spatial attention. \textcite{yarbus1967eye} wrote: 

\begin{quotation}
\footnotesize{„Eye-movements reflect the human thought processes; so the observer‘s thoughts may be followed to some extent from records of eye-movements (the thought accompanying the examination of the particular object). It is easy to determine from these records which elements attract the observer‘s eye (and, consequently, his thought), in what order, and how often.“}
\end{quotation}

Following this reasoning, the comparison of scanpaths of different subjects constitutes a measure of similarity in the different subjects’ attentional processes and adds a useful dimension to the traditional analysis of eye-tracking data. \newline
In recent years, many approaches for scanpath comparisons were developed and implemented in various software solutions. Among them are methods based on (semantic) areas of interest (AOIs) such as the \textit{Levenshtein distance} (\cite{levenshtein1966binary}), or an improved generalization of it in \textit{ScanMatch} (\cite{cristino2010scanmatch}), methods based on attention maps such as \textit{AMAP} (\cite{ouerhani2004empirical}), methods employing machine learning algorithms such as \textit{SMAC with HMM} (\cite{coutrot2018scanpath}), or methods based on vector-geometry such as \textit{MultiMatch} (\cite{jarodzka2010vector}). The latter method is a multidimensional approach of scanpath comparison on five different measures similarity, \textbf{shape}, \textbf{length}, \textbf{position}, \textbf{direction}, and \textbf{fixation durations}. According to \textcite{jarodzka2010vector}, it has been developed specifically to overcome known shortcomings of many previous methods that limit their informative value. These shortcomings amount to a loss of information due to the use of coarse aggregate measures of eye movement, susceptibility of the results of a comparison to influential data points due to arbitrary classifications, or both. The main reason it overcomes these deficiency is that it does not rely on AOIs, but the precise locations in pixels in two-dimensional space, which leads to a finer level of detail in scanpath comparison. \newline A number of studies used the method in evaluations or applications of scanpath comparisons. In evaluations with simulated and actual eye-tracking data, the method was found to be robust against spatial noise, sensitive to position, order and fixation duration, and outperformed the ScanMatch method in AOI border cases (\cite{dewhurst2012depends}). In a comprehensive test of eleven common scanpath comparison methods on static real-life photographies, MultiMatch was found to be robust in inter- and intra-subject comparisons of scanpaths (\cite{anderson2015comparison}). As it was also used to study mental activity involved in perceiving visual input by others (\cite{french2017evaluation}) already, the MultiMatch method was chosen to be employed in the current thesis to derive a measure for attentional processes of participants during movie watching. \newline
For its computation, this attentional measure utilizes the scanpath information of all participants to obtain a common measure of attentional modulation of the stimulus in the sample. It follows a reasoning conceived by \textbf{Baumgartner et al. (unpublished manuscript)}: The more similar all participants scanpaths are for any given scene of a movie stimulus, the more exogenous control is exercised by this movie segment. \textbf{TODO: this might need additional details/motivation/examples} \newline
 An overview of the method is available in section \ref{section:multimatch}.

\section{Investigating functional specificity with functional sensitivity profiles}\label{section:methodintroduction}

In addition to using scanpath comparisons for a measure of visual attention, a novel method to investigate the functional specificity of regions of interest is developed and implemented in this thesis. As outlined in section \ref{section:shortcomings}, a general approach of localization consists of contrasting experimental conditions known to activate specific functional regions in a GLM, for example the contrast between activation during static images of houses and static images of faces to localize the FFA (\cite{fox2009defining}), or the use of instructed horizontal saccades versus fixations to localize the FEF (\cite{connolly2002human}). This approach suffers from a number of drawbacks: \newline
First, as outlined as well in \ref{section:shortcomings}, the employed artificial stimulation is not ecologically valid, which decreases the sensitivity of functional localizers and thus their applicability across studies. Secondly, a reliance on narrow stimulus categories and simple contrasts omits insights into further stimulus aspects of potential relevance - especially since an exact contrast between conditions accounting for various ROI-specific factors is not known ahead of time, and thus cannot be incorporated into the localizer GLM contrast. Moreover, it is challenging to differentiate functionally related but different ROIs with a simplistic experimental design. Therefore, localization often further requires to take spatial information for functionally close regions (such as the occipital face area (OFA) and the FFA) into account.
Lastly, changes in statistical thresholds can alter localization results substantially (\cite{fox2009defining}) and introduce an additional source of \textit{researchers degrees of freedom} into an analysis. Obtaining additional insights in regard to functionality of regions of interest might be beneficial to reduce reliance on spatial information, subjective thresholding, and may additionally give rise to new insights into the functional neuroanatomy of ROIs.\newline
The approach introduced in this thesis is designed to estimate a maximally discriminative contrast between the functional signatures, i.e. the activation time courses, of any two ROIs in question, and to use the available experimental design - regardless of complexity - to provide a functional description for it. The results of this analysis should give differential insights into the functional specificity of ROIs beyond simple contrasts of stimulus conditions. \newline
The method relies on sensitivity profiles derived during a classification analysis. A short introduction into the basic principles of this type of machine learning tool and and the chosen algorithm for this analysis will be introduced in the next subsection, followed by an elaboration on how the derived sensitivities are further utilized. In order to validate the method, its applicability and generalization capabilities were first tested on a general localization paradigm, and later movie-data, for already localized ROIs, which will be elaborated on in section \ref{section:methodvalidation}.



\subsection{Classification analysis}
The term \textit{classification analysis} is most often used in the context of \textit{machine learning}. Machine learning refers to the study and construction of software that can learn from data without being explicitly programmed (\cite{zeigermann2018machine}). Especially in the data-rich environment of neuroimaging, machine learning algorithms and techniques have gained immensely in popularity and significance as for their ability to detect hidden patterns and trends in the data (\cite{vogt2018machine}). One particular type of tool from machine learning is a so called \textit{classifier}, which is used for a classification analysis. In such an analysis, a classifier is trained on a subset of data, the \textit{training set}, and is then used for predictions on a previously unseen subset of data, the \textit{test set} (\cite{zeigermann2018machine}). \newline
More formally, a classifier is a function created from training data to predict the value of a class attribute $C \in \{1, \ldots, r\}$ (the \textit{label}), given the predictive \textit{features} $X = (X_1, \ldots, X_d)$  with values in $S = \{S_1 \times \ldots \times S_d\}$ and the sample $x = (x_1, \ldots, x_d)$. Suppose $(X, C)$ is a random vector with a joint feature-label probability distribution $p(x, c)$. A classifier $\psi$ is the function that maps X onto C:

\begin{equation}\label{classifier}
\psi:\{ S_1, \times \ldots, \times S_d \} \rightarrow \{1, ..., r\}
\end{equation}
\begin{equation}
x \mapsto c.
\end{equation}


The overall aim of the classification analysis in this thesis was not the classification in general, but to derive a sensitivity measure in the form of feature weights from a simultaneous sensitivity analysis. There are a number of different classification algorithms available to build classification models, and based on prior work of \textbf{Nastase et al. (2016)}, a \textit{linear Gaussian Naive Bayes} classifier (GNB) was further explored due to its high classification accuracy and easy interpretability of feature weights in case of linearity. \newline
The algorithm belongs to the family of Naive Bayes algorithms. These algorithms use a probabilistic approach to classification, utilizing the Bayes theorem to capture the relationships within input data and output label. Given some samples $x_1, \ldots, x_n$ with a number of characteristics on different features, and labels $c_1, \ldots, c_r$, naive Bayes classifiers learn a model of the joint probability, $p(x, c)$ during training. During testing, they make predictions by using Bayes theorem to calculate the probability of a label, given the feature characteristics of a sample, $p(c|x)$. Two fundamental assumptions exist for these algorithms: \newline 
\begin{itemize}
	\item \textit{Conditional independence}: Each feature $x_i$ is independent of every other feature $x_j$ for $j \neq i$ given the outcome $c_r$.
	\item \textit{Equal contribution of features}:  Each feature is given the same weight for the outcome prediction. 
\end{itemize} 
A linear Gaussian Naive Bayes classifier further assumes the data to be distributed according to a Gaussian distribution, and it assumes the feature variance to be independent of class (i.e. $\sigma_{i, 0}^2 = \sigma_{i, 1}^2$). Given the latter assumption holds, the decision boundery between any two classes 1 and 0,
\begin{equation}
\prod_{i=1}^{d} P(X_i |C = 0)P(C=0) = \prod_{i=1}^{d} P(X_i |C = 1)P(C=1),
\end{equation}
is linear (for the mathematical proof, see e.g. \textcite{DBLP:books/daglib/0087929}), chapter 3). The feature weights of this classifier are defined as
\begin{equation}
w_i = \sum_{i}\frac{\mu_{i, 0}-\mu_{i, 1}}{\sigma_{i}^{2}}.
\end{equation}

{\textbf{TODO: Draw a picture with a linear decision boundary between two classes with two features, plus the Gaussian distributions and equal variances, just for clarity. this part is not comprehensible at all for anyone not in the topic...there are means to get this done in Latex, packages are imported already, but might be time consuming...or plot it with pymvpa...  for now the gimp one suffices.
Steal idea for visualization from: \newline https://www.youtube.com/watch?v=TcAQKPgymLE, just make it linear.}
\newline
Figure \ref{GNB} illustrates the case of two features and two classes with their respective distribution. The feature weight of the feature $A$ can be derived as the difference in means given the class, $\mu_{A, 1} - \mu_{A, 2}$, divided by their common variance.
\begin{figure}[H]
	\includegraphics[scale=0.5]{img/GNB.png}
	\caption[A linear decision boundery for a linear Gaussian Naive Bayes classifier]{\small{Exemplary illustration of a linear decision boundary in the case of equal variances. The distribution of each features data given the class follows a Gaussian distribution with mean $\mu_i$ and variance $\sigma^2$.}}
	\label{GNB}
\end{figure}

\subsection{A "transposed" approach to the functional sensitivity of ROIs}
The method developed here utilizes the sensitivites obtained during a classification analysis and thus extends work by \textbf{Nastase et al. (2016)}: Following a classification of voxels (samples) to ROIs (labels) based on the observed data in the ROIs from other participants, a subsequent analysis of the classifiers sensitivity profile obtained from a decision between ROI pairs is used to shed light onto their functional distinctiveness. As opposed to the common approaches at classification with subsequent sensitivity analysis in neuroimaging (e.g. as used in \textcite{poldrack2009decoding}), the datasets are transposed: The features in this classification problem are time points, while voxels constitute samples. \newline
In a first analysis step, a sensitivity profile of two ROIs is obtained via classification with the linear GNB. As the dataset is transposed to have time points as features, this sensitivity profile is a time course of sensitivities: each weight corresponds to the difference in mean activation of all voxels given the label (ROI) at one time point, normalized by their common variance. In an examplary decision between ROIs A and B, a negative weight corresponds to an overall higher activation in B at this particular point in time. In a second step, this time course is entered as a criterion in a GLM with details about the experimental design at different time points as regressors. The resulting coefficients indicate the importance for each regressor (i.e. stimulus characteristic) for the distinction between ROIs. Section \ref{section:methodvalidation} will give a comprehensive example of this approach. \newline
\textbf{ TODO: [HOW CAN WE USE THIS FOR FEF]}


\chapter{Methods}\label{section:methods}

The following method section attempts to give a concise description of this thesis' data basis, fMRI and eye-tracking, and the employed and developed methods, MultiMatch and ROI specificity analysis.  \textbf{[TODO: ... and whats presumably the main part of this thesis, the FEF thingy]}

\section{Data basis}

Data stems from the 2016 released extension of the studyforrest dataset\footnote{all data and code are publicly available at https://github.com/psychoinformatics-de/studyforrest-data-phase2.} (\cite{hanke2016studyforrest}; \cite{sengupta2016studyforrest}). In this extension, $N = 15$ right-handed participants (age range 21 – 39 years, mean age 29.4 years, six female, normal or corrected-to-normal vision), who had previously participated in the studyforrest project, watched the audio-visual movie “Forrest Gump” (R. Zemeckis, Paramount Pictures, 1994) during simultaneous fMRI and eye-tracking recording. They further underwent a traditional localizer paradigm for higher visual areas: The fusiform face area (FFA), the occipital face area (OFA), the extrastriate body area (EBA), the hippocampal place area (PPA), the lateral occipital complex (LOC) and early visual cortex (\cite{sengupta2016studyforrest}).

\subsection{Stimulus material}

Stimulus material for the audiovisual movie was the German dubbed version of Forrest Gump, overlaid with a German audio-description (\textbf{Koop, Michalski, Beckmann, Meinhardt \& Benecke}, produced by Bayrischer Rundfunk, 2009), originally broadcast as an additional audio track for visually impaired recipients on Swiss public television. The additional audio track consists of a male narrator describing the visual content of a scene between dialogs, off-screen-speech or any other relevant audio-content of the movie (\cite{hanke2014high}). The video track for the movie stimulus was re-encoded from Blu-ray into H.264 video (1280 x 720 at 25 frames per second (fps)). In accordance to the procedure in an earlier phase of the studyforrest project, the movie was shortened by removing a few scenes less relevant for the major plot to keep the fMRI recording session under two hours. The shortened movie was then segmented into eight segments of roughly 15 minutes of length (for an overview on segment duration, final stimulus content and detailed procedures see \textcite{hanke2014high}). Stimulus material for the localizer paradigm consisted of 24 unique gray-scale images for each of six different categories (faces, bodies, houses, scenes, everyday objects, and scrambled images) at a resolution of 400 x 400px, matched in luminance, and displayed at 10 x 10$^\circ$ of visual angle (\cite{sengupta2016studyforrest}).


\subsection{Procedures}
Functional MRI data acquisition for the audio visual movie was undertaken in two consecutive recording sessions on the same day, with a break of flexible duration. Within each session, four movie segments were presented in chronological order (\cite{hanke2016studyforrest}). Visual stimuli were projected on to a screen inside the bore of the magnet using an LCD projector, and presented to the subjects thought a front-reflective mirror on top of the head coil at a viewing distance of 63cm. The screen dimensions were 26.5cm x 21.2cm (corresponding to 1280 x 1024 px) at a resolution of 720p at full width, with a 60Hz video refresh rate (\cite{sengupta2016studyforrest}). Eye-tracking was performed with an Eyelink 1000 (software version 4.594) using monocular corneal reflection and pupil tracking with a temporal resolution of eye gaze recordings of 1000Hz. The camera was mounted at an approximate distance of 100cm to the left eye of subjects, which was illuminated
by an infrared light source (\cite{hanke2016studyforrest}). Movie presentation and eye-tracking were synchronized by starting the eye gaze recording as soon as the stimulus computer received the first fMRI trigger signal. Timings of subsequent trigger pulses and onsets of every movie frame
were logged. Using a 3 Tesla Philips Achieve dStream MRI scanner with a 32 channel head coil, T2*-weighted echo-planar images (gradient-echo, TR = 2s, echo time = 30ms, flip angle = 90) were acquired during movie watching. For the eight segments, 451, 441, 438, 488, 462, 439, 542, and 338 volumes were acquired, respectively (\cite{hanke2016studyforrest}).\newline

In the localizer paradigm, participants viewed the different object categories in a total of four block-design runs, with two 16s blocks per stimulus category. The order of the individual images per category differed across runs and participants, but the sequence of category blocks was the same for every run and participant. Localizers for higher visual areas (FFA, OFA, PPA, LOC, EBA, and early visual cortex) were obtained by the original authors of the studyforrest extension by means of a two-level GLM analysis on contrasts of activation between object categories (for details, see \cite{sengupta2016studyforrest}).

\subsection{Software}
Data preprocessing and analysis were conducted with a multitude of Python-based software packages. Preprocessing of data relied primarily on custom made workflows written with nipype (\cite{gorgolewski_krzysztof}), and utilizing FSL (\cite{jenkinson2012fsl}). Much of the data analysis was done using pyMVPA (\cite{hanke2009pymvpa}). Datalad, a version control tool for scientific data (https://zenodo.org/record/1470735), served as a backbone for all own work presented in this thesis. If applicable and unless otherwise stated, the alpha level for statistical significance was set to $\alpha = 0.05$

\subsection{Preprocessing}
Eye-tracking data were normalized such that all gaze coordinates are in native movie frame pixels, with the top-left corner of the movie frame located at (0, 0) and the lower-right corner located at (1280, 546) (\cite{hanke2016studyforrest}). The amount of unusable data, primarily due to signal loss due to eye blinks, ranged from less than 1 to 15\% for 13 of the 15 in-scanner subjects (the other two subjects’ data contained 85 and 36\% of data loss, respectively). In-scanner acquisition had an approximate spatial uncertainty of 40px according to the calibration procedure. \newline
The raw eye-tracking data was classified into different eye movements. For this, based on an adaptive, velocity-based algorithm proposed by \textcite{nystrom2010adaptive}, \textbf{Dar et al. (in preparation)} implemented a data-driven algorithm for robust eye movement detection for natural viewing (REMoDNaV) in Python\footnote{The sourcecode can be found at github.com/psychoinformatics-de/remodnav.}. All results of this algorithm are publicly available at [\textbf{TODO: }URL here]. The algorithm categorizes the raw data into the eye movement categories saccades, fixations, post-saccadic oscillations (glissades), and disregards any unclassifiable data (such as blinks). The eye events are reported together with their start- and end coordinates, their onsets and duration in seconds, their velocity and the average pupil size.

Motion-corrected fMRI data in a study-specific group-template were obtained from the respective Github repositories via datalad \textbf{[CITE? Zenodo thing is ugly]}. In a first step, both datasets, localizer and movie-data, were preprocessed identically using a custom made nipype workflow, consisting of whole-brain masking, high-pass filtering and smoothing with a Gaussian kernel of 4mm FWHM\footnote{All code for preprocessing and analysis can be found at https://github.com/AdinaWagner/localizer.}. \newline \textbf{TODO: this might need to become a tiny bit more detailed/informative}

\subsection{Extraction and processing of eye-tracking data}

For the extraction of saccadic eye events, the results were filtered to include only saccades. For the computation of event files following the BIDS standard (\cite{gorgolewski2016brain}), the Cartesian coordinates of the saccades were transformed into Polar coordinates (angle and length, a graphic explanation of this procedure can be found in \ref{fig:Polar_to_cartesian}, B). Based on their angle, saccades were classed into event types describing eight different orientations in the visual field. Saccade onsets and durations were extracted from the REMoDNaV output. The resulting event file per run hence contained the onset, duration, amplitude (length) and direction of saccades. \newline \textbf{TODO: and now what did we do with it?}
\textbf{TODO: At one point this might just be integrated into another method section}

\section{MultiMatch}\label{section:multimatch}

The MultiMatch method (\cite{jarodzka2010vector}) was selected as a promising approach of scanpath comparison. The original implementation, however, only existed as a Matlab toolbox shared upon request by the corresponding author of the respective publication (\cite{dewhurst2012depends}). This reliance on closed-source software and barriers in retrieval of the software\footnote{While acquisition of the toolbox via e-mail was prompt and complication-free, later correspondence with several of the authors was an odyssey of expired email addresses, changed affiliations and digital communication allegedly lost in the void of the Internet.} hinder the wider usage and potential improvement of the method. A publicly available, open-source implementation would instead facilitate its usage in scientific works. Therefore, the method was ported from Matlab into Python and transformed into the pip-installable module \textit{multimatch}\footnote{The module can be installed via ``pip install multimatch``. A corresponding publication (Wagner \& Hanke) in the Journal of Open Source Software (JOSS) in currently in preparation. The sourcecode can be found at github.com/AdinaWagner/multimatch.}.

\subsection{multimatch in Python}
The following section gives a brief overview of the general MultiMatch method. An overview in pseudocode is outlined in algorithm \ref{algo:multimatch}. \newline 
The method takes two n x 3 fixation vectors of two scanpaths with x-coordinates in pixel, y-coordinates in pixel, and duration of fixations as columns as its input. Based on the coordinates and durations of fixations, the scanpaths are represented as geometric vector sequences as shown in figure \ref{fig:Polar_to_cartesian}:

\begin{figure}[H]
		\includegraphics[scale=0.35]{img/scanpathconversion.png}
		\caption[Geometric representation of eye movements]
		{\small{\textit{A: Geometric scanpath representation.} An idealized saccade is represented as the shortest distance between two fixations. The Cartesian coordinates (x, y) of the fixations are thus the starting and ending points of a saccade. The length of a saccade in x (or y) direction is computed as the difference in x (or y) coordinates of starting and ending point. \newline
		\textit{B: Length and angle computation.} Length from the coordinate origin, rho, is computed as the Euclidean norm by means of the Pythagorean theorem: $r = \sqrt{ x^2 + y^2}$. The angle in radians, theta, is computed as a variation of the arctangent function as the angle from positive x axis to the point (x, y), with positive values denoting counterclockwise angle from positive x axis and negative values denoting clockwise angles: $\theta = arctan2(x, y)$.}}
		\label{fig:Polar_to_cartesian}
\end{figure}

To reduce complexity, the scanpaths are simplified according to angle and amplitude in an iterative procedure. Two or more saccades are grouped together if angles between two consecutive saccades are below an angular threshold \textit{TDir}, or if the amplitude of successive saccades is below a length threshold \textit{TAmp}, as long as intermediate fixations of the saccades are shorter than a duration threshold, \textit{TDur}. As such, small, locally contained saccades, and saccades in the same general direction are summed to form larger, less complex saccades. \newline
In order to find pairings of saccade vectors to compare, the simplified scanpaths are temporally aligned. The aim is to not necessarily align two saccade vectors that constitute the same component in  their respective vector sequence, but those two vectors that are the most similar while still preserving temporal order. In this way, a stray saccade in one of the two scanpaths does not lead to an overall low similarity rating, and it is further possible to compare scanpaths of unequal length.  To do so, all possible pairings of saccades are evaluated in similarity by the vector differences between all pairings (shape): The vector differences between each element $i$ in scanpath $S1 = {u_1, u_2, \ldots, u_m}$ and each element $j$ in scanpath $S2 = {v_1, v_2, \ldots, v_n}$ are computed and stored in a matrix $M$ with dimension m x n as weights that denote similarity, with low weights corresponding to high similarity. In a next step, an adjacency matrix is build, defining the rules on which connection between matrix elements are allowed to preserve the temporal order of saccades: In order to take temporal sequence of saccades into account, connections can only be made to the right, below or below-right (green arrows in figure \ref{fig:directedgraph}). Together, matrices $M$ and the adjacency matrix constitute a matrix representation of a directed, weighted graph (figure \ref{fig:directedgraph}). Elements of the matrix are nodes, the connection rules constitute edges and the weights define the cost associated with each connection. 
In the generic example in figure \ref{fig:directedgraph} the edge between node (1, 1) and node (1, 2) has an associated weight, or cost, of $w_2$. 


\begin{figure}
	\includegraphics[scale=0.4]{img/weightedgraph.png}
	\caption[Scanpath alignment as a shortest-path problem.]
	{\small{The elements of two hypothetical scanpaths (left) are used to compute vector differences between all possible pairings as weights $w_1, \ldots, w_6$ in a matrix $M$ (right). Green arrows indicate connection rules defined by an adjacency matrix. Taken from \textcite{jarodzka2010vector}}}
	\label{fig:directedgraph}
\end{figure}

A Dijkstra algorithm (\cite{dijkstra1959note}) is used to find the shortest path from the top left node, the first two saccade vectors, to the bottom right node, the last two saccade vectors. “Shortest” path is defined as the connection between nodes with the lowest possible sum of weights, i.e. the highest similarity. The path returned by the Dijkstra algorithm is a sequence of indexes, denoting pairings of saccade vectors from each scanpath, and as such the desired alignment of scanpaths (\cite{dewhurst2012depends}).  Finally, in a last step, five measures of scanpath similarity (see figure \ref{fig:simmeasures}) are computed for a multidimensional similarity evaluation. This is done by performing simple vector arithmetic on all aligned saccade pairs $(u_i, v_j)$, normalizing the results to range [0, 1] according to a certain metric, and taking the median of the results. Higher values indicate higher similarity between scanpaths on the given dimension. \newline

\begin{figure}[H]
	\includegraphics[scale=0.5]{img/simmeasures.png}
	\caption[Similarity measures]{\small{\textbf{Shape}: Vector difference between aligned scanpaths, normalised by 2x the screen diagonal. \textbf{Length}: Difference in vector lengths, normalized by the screen diagonal. \textbf{Direction}: Angular distance between saccade vectors, normalized by $\pi$. \textbf{Position}: Position difference of fixation, normalized by the screen diagonal. \textbf{Duration}: Duration differences of aligned fixations, normalized against maximum duration within the comparison.}}
	\label{fig:simmeasures}
\end{figure}


\begin{algorithm}[H]
	\begin{small}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\Input{n x 3 fixation vectors (x, y, duration), simplification thresholds $T_{Amp}$, $T_{Dir}$, $T_{Dur}$}
	\Output{eventfile: onset of scanpath, duration, and five similarity measures range [0, 1]}
	\For{\textbf{each} fixvector}
	{ 
		\For{\textbf{each} fixation \textit{f} in fixvector}
		{
			fixation$_x$, fixation$_y$, fixation$_{dur}$ = $x, y, duration$ \;
			saccade$_{x(y)}$ = fixation$_{x(y)}$$_{f}$ $-$ fixation$_{x(y)}$$_{f+1}$ \;
			saccade$_{lenx(leny)}$ = fixation$_{x(y)}$ $-$ saccade$_{x(y)}$\;
			saccade$_{rho}$, saccade$_{theta}$ = cartesian2polar(saccade$_{lenx}$, saccade$_{leny})$ \;
			\textbf{eyedata} = [fixation$_x$, fixation$_y$, fixation$_{dur}$, saccade$_x$, saccade$_y$, \newline saccade$_{lenx}$, saccade$_{leny}$, saccade$_{rho}$, saccade$_{theta}$]
		}
		\While{simplification is still possible}
		{
			\For{\textbf{each} saccade i in eyedata}
			{
			\If{angle(saccade$_{i}$, saccade$_{i+1}) < TDir$ \bfseries{and} $fixation_{dur} < TDur$}
			{combine saccades}
			\If{saccade$_{rho}$\space$_i$, saccade$_{rho}$\space$_{i+1}$) < TAmp \textbf{and} fixation$_{dur}$ $<$ TDur}{combine saccades}
			}
		}
	}
		\For{\textbf{each} pair of (simplified) scanpaths $S1 = \{u_1, \ldots, u_m\}, S2 = \{v_1, \ldots, v_n\}$}
		{
			calculate M: Matrix of saccade length differences between each $i \in S1$ and  $j \in S2$\;
			create a directed graph with elements of M as weights $w_1, \ldots, w_i$\;
			align scanpaths with $min(\sum w)$ from $M(0,0)$ to $M(n,m)$ via Dijkstra algorithm\;
		
		add aligned scanpath to set of aligned scanpaths \;

		\For{ \textbf{each} pair($u_i,v_i$) in aligned scanpaths}
		{
			angular difference = angle($u_i, v_1$)\;
			position difference = $\sqrt{(x(u_i) - x(v_i))^2 + (y(u_i) - y(v_i))^2}$ \;
			vector difference = $\sqrt{(len\_x(u_i) - len\_x(v_i))^2 + (len\_y(u_i) - len\_y(v_i))^2}$\;
			length difference = $abs(rho(u_i) - rho(v_i))$\;
			duration difference = $abs(dur(u_i)-dur(v_i))$ \;
		}
		normalize angular difference by $\pi$\;
		normalize vector difference by 2x the screen diagonal (maximum theoretical distance)\;
		normalize length difference, position difference by the screen diagonal\;
		normalize duration difference by maximal duration difference in the scanpaths\;
		\textbf{return} median for each similarity measure
	\textbf{return} \textbf{similarity measures}
}
	\caption{multimatch}
	\label{algo:multimatch}
\end{small}
\end{algorithm}
\textbf{TODO: Not sure whether the pseudo code makes anything any clearer. Looks neat though and took a while. Might ban it into Appendix.}

\subsection{multimatch usage in the studyforrest dataset}
The extraction of eye events for further use with multimatch to estimate a measure of exogenous attentional control included several considerations based on findings and known caveats in eye-tracking and attention research with dynamic scenes. The resulting additional functions are mainly concerned with automatic scanpath extraction from the approximately 15 minutes spanning and several eye-movement categories containing eye-tracking event files by user-defined rules. An overview in pseudocode can be found in algorithm \ref{algo:multimatch_forrest}. \newline
First, as the stimulus was dynamic, the start and end of pursuit movements were relabeled as fixations. This was done to accommodate the fact that most areas of interest in a movie are in motion. Focusing on the running young Forrest for example would likely appear as a slow pursuit in the eye event data. Disregarding such pursuits would lead to a large loss in data and variability, and as such, they were included via relabeling. \newline A different consideration concerned the selection of scanpaths from the approximately 15 minute long segments, as they are unsuitably long to derive a continuous attention measure or do a scanpath comparison on. It has been shown that subjects gazes have a bias towards the center in Hollywood movies (\cite{tseng2009quantifying}). This bias can at least in part be traced back to a strong center bias directly after cuts in dynamic scenes. \textcite{carmi2006visual} found the highest degree of exogenous attentional control (i.e. maximal inter-observer similarity in gaze as recorded via eye-tracking) in a moving collage of 50 heterogeneous video clips directly after cuts to new, semantically unrelated scenes. Without isolating cuts it therefore can not be disentangled whether the contribution of visual features of the movie on scanpaths stems from exogenous control from a movie element or from the sudden occurrence of a new scene after a cut. To not introduce such a confound in the eye-tracking data, the data was cut into segments that did not contain cuts to different scenes, relying on a location annotation by \textcite{hausler2016annotation}. They define a \textit{shot} as a movie sequence between two cuts. The location annotation contained a detailed description of the timing of each of the 870 shots of the movie as well as information about the depicted location in different levels of abstraction. In a first step, all consecutive short shots to the same locale (e.g. two consecutive, short movie shots in Forrest's bedroom, without any cuts containing semantically unrelated content) were grouped together. A “short” shot was defined as being shorter than the median length of 4.92s. This procedure increased the median length of shots from 4.92s to 7.019s. These elongated shots are henceforth referred to as \textit{snippets}. In a next step, within the scanpath comparison, the onset and offset times of the resulting snippets were extracted for every snippet longer than 4.92s. To not introduce an influence of snippet length into the similarity results, as longer movie snippets will be more likely to be less similar than short ones, scanpath lengths were standardized to approximately 4.92s seconds (the original median length). To further evade any problems associated with the center bias, scanpaths were extracted from the end of the snippet: The last oculomotor event within the range of the snippet marked the end of a scanpath. As such, scanpaths began maximally distant to the snippet onset.
As the eye movements of participants are interindividually different and do not correspond exactly to snippet timing, the precise onsets and resulting exact durations were computed as well, and stored for later use as regressors.

\begin{algorithm}[H]
	\begin{small}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	
	\Input{REMoDNaV eye movement datafiles per run, movie location annotation per run, desired length to elongate shots to \textit{ldur}, desired scanpath length \textit{dur}}
	\Output{One n x 3 fixation vector (x, y, duration) for any snippet longer than $dur$}
	{
		\If{$event_{row}$ = pursuit}
		{
			regard start and end points of pursuit as fixation
		}
	}
	\For{\textbf{each} row in REMoDNaV datafile}
	{
		\If{$event_{row}$ = fixation}{
			extract x-, y-coordinate, duration}
		\If{x, y < 0 or x > 1280 or y < 720}{discard as out-of-bound gaze}
	}
	\For{\textbf{each} row in annotation}
	{
		\If{($locale_{row} = locale_{row+1}$) and ($duration_{row}, duration_{row+1} < ldur$)}
		{combine to one shot}
	}
	\For{\textbf{each} row in annotation}
	{
		\If{$duration_{row}$ > dur)}
		{extract shotonset and offset time}
	}
	\For{ \textbf{each} i in length(onset)}
	{fixvector$_i$ = REMoDNaV[REMoDNaV$_{onset} > onset_i$; REMoDNaV$_{onset}$ < offset$_i$]
	}
	return $\{$fixvector$_1$, \ldots, fixvector$_i$$\}$, onsets, durations
	\caption{The studyforrest specific functions of multimatch}
	\label{algo:multimatch_forrest}
	\end{small}
\end{algorithm}
\textbf{TODO: Not sure whether the pseudo code makes anything any clearer. Looks neat though and took a while. Might ban it into Appendix.}

\subsection{Deriving a measure of attentional modulation from multimatch}
A common measure of attentional modulation was computed in a two-step procedure. First, scanpath comparisons of all scanpaths from the same shot of two subjects were calculated for all possible pairs of subject. This resulted in $C_k(n) = {N\choose k} = {15\choose 2} = 105$ combinations for N = 15 subjects. These comparisons were done without any further simplification (i.e. no use of the direction, length, and duration thresholds), as even minor differences in scanpaths obtained from a movie can correspond to major differences in attended visual stimuli. In a second step, the resulting similarities for each of the five similarity dimensions were averaged. Thus, for each snippet longer than 4.92s five similarity measures were computed that represented the average similarity of scanpaths of all subjects on the given dimension. 

\section{A novel approach for the analysis of ROI specificity}\label{newmethod}
This section outlines the proposed method for ROI specificity analysis in general, and provides examplary use cases in the form of two sequential preliminary validation analysis, independent of the FEF. \newline
The general idea behind the proposed method is to derive a functional signature as a temporal course from two ROIs in question, and to explain the functional signature by providing a detailed description of the experimental design. This is realized with a two step procedure consisting of (1) classification and (2) GLM analysis. 
Simultaneously, the method relies on transposing the fMRI dataset, such that all time points are features, and the voxels are samples.  \newline
The functional signature is obtained from a classifiers' sensitivity profile in an n-fold leave-one-participant-out cross-validation: The classifier is trained on all subjects except for one and then tested on that out-of-sample individual, and this method is repeated for each of the subjects (step 1, classification analysis). During classification, feature weights are obtained for the decision between all pairwise combinations of classes. Obtained sensitivity profiles per fold are averaged across folds. As a result, there is one temporal sensitivity profile for each pairwise ROI combination. The achieved overall classification accuracy serves as a measure of whether population wide functional specificity and sensitivity between the ROIs can be captured. In the second step (GLM), the averaged sensitivity profile is regressed onto the stimulus description for an estimate on how well the experimental design could describe ROI differences, and to point to the effects in the experimental design contributing to an ROI distinction as evident from the beta coefficients of the respective regressors. \newline
The classifier of choice, a Gaussian Naive Bayes classifier, was readily available in pyMVPA’s (\cite{hanke2009pymvpa}) mvpa2.suite python module as GNB() and customizable to be a linear classifier with the parameter \textit{common\_variance = True}. However, it lacked functionality to compute feature weights for a sensitivity analysis. Therefore, as a preliminary step, the GNB class in the sourcecode of the pymvpa package was extended with the implementation of the subclass \textit{GNBWeights}. This made a sensitivity analysis possible, that returns the weights $w_i = \sum_{i}\frac{\mu_{i, 0}-\mu_{i, 1}}{\sigma_{i}^{2}}$ for each feature for any possible pair of labels in a binary decision. For an example, consider a classification task with three labels L0, L1, L2, and two features, F1, F2. The implemented sensitivity analyzer will for each possible pair of labels (L0 versus L1, L0 versus L2, L1 versus L2) return one weight per feature\footnote{The sourcecode for this change can currently be found in the following PR: https://github.com/PyMVPA/PyMVPA/pull/591/files}. \newline
As the datasets need to be transposed to have time points as features, each weight corresponded to the difference in mean activation of all voxels given the class at one time point, normalized by their common variance. In case of for example the decision between FFA and PPA, a negative weight therefore corresponds to an overall higher PPA activation at this particular point in time. The type of stimulation present at this point in the experimental design likely contributes to the distinction between the two areas, and the magnitude of its influence will be visible in its associated beta coefficient obtained from the GLM. \newline
 

\subsection{Method validation}\label{section:methodvalidation}

In order to establish feasibility of the procedure and to test the generalization of the developed approach, the method was validated in a two-step procedure. As a test of plausibility, it was used on data from a common block design for ROI localization within the studyforrest dataset. As a measure of its capabilities to generalize to naturalistic stimulation, it was subsequently used with the studyforrests' movie data, collected on the same participants. For both datasets, six functional ROI masks (FFA, PPA, OFA, EBA, LOC, early visual cortex) were available to serve as labels during classification analysis. Both validating analyses were restricted to the FFA-PPA distinction as for the wealth of literature concerned with these ROIs. \newline
For use in the localizer dataset, the event files describing the stimulation protocol of the object-category task were used to compute a group-level event file. This was possible because the order of blocks was the same across runs and participants (see \cite{sengupta2016studyforrest}). For each participant, the onsets of each image were averaged. Deviations from individual onsets to the average group onsets were smaller than 0.5 seconds. In addition to the six object categories, the first occurrence of an image within an object category was classed a new type of event with the suffix 'first', resulting in 12 regressors (face, face\_first, body, body\_first, house, house\_first, scene, scene\_first, object, object\_first, scramble, scramble\_first). This was done to account for any additional variation introduced by the change in categories. Using pyMVPAs fit\_event\_hrf\_model() function, the resulting events were entered as regressors to explain the time course of sensitivities. \newline
For generalization in the movie dataset, a stimulus description of the movie was obtained from two sources, the location annotation of \textcite{hausler2016annotation}, and an automatic facial feature extraction per frame of Hanke (unpublished, computed using Google's Cloud Vision API via pliers (\textbf{McNamara et al. 2017})). An event was derived for each "setting" in the location annotation occurring more than once. An event was further derived for night-time, a scene being located exterior, a general cut, and contextual jumps to future or past from the previous shot. The facial feature data was downsampled to 1 second and two conditions were derived, "face" for any 1 second time window with a maximum of less than 3 faces, and "many\_faces" for any 1 second time window with more faces present.

\section{TODO: FEF}
\textbf{[this is a gigantic TODO]}

\subsection{Localization of the FEF}
\begin{itemize}
	\item Extraction of saccadic eye movements from remodnav data. 
	\item Selection of saccades within 30$^\circ$ of the horizontal and vertical axis, see figure \ref{fig:saccs}
	\item Implementation of a glm with fitlins (cite). As fitlins was in early alpha stage during the completion of the analysis, some enhancements where suggested and implemented as pull requests.
\end{itemize}

\begin{figure}[H]
	\includegraphics[scale = 0.7]{img/saccades.png}
	\caption[saccadic eye movements during movie watching]{\begin{small}Polar coordinates (direction in degrees on the outer ring, length in visual degrees on the inner grid) of saccadic eyemovements of all subjects during movie watching. Colors indicate subjects. Yellow overlay indicates 30$^\circ$ around the horizontal or vertical axis.\end{small}}
	\label{fig:saccs}
\end{figure}

add something in the notion of: Study of the frontal eye fields attempting to capture its involvement as both a structure of motion and attentional deployment ... somewhere


\chapter{Results}\label{section:results}

\section{Eye movements and multimatch}
In order to check the validity of the eye-movement extraction with REMoDNaV, the mean and median duration of extracted fixations were compared to the results of \textcite{dorr2010variability}. Fixations as found by REMoDNaV had a length of mean = 0.326s, median = 0.256s, which corresponds closely to the aforementioned study's results (0.354s and 0.253s, respectively). A comparison of algorithm performance and human coding of eye movements on data of \textcite{andersson2017one} by \textbf{Dar et al. (in preparation)} showed the highest degree of concordance of all contemporary algorithms used in \textcite{anderson2015comparison}, providing further evidence for the validity of extracted eye movements. \newline

Based on the REMoDNaV output, the multimatch method was able to extract a total of N = 533 scanpaths from the movie. The median duration of extracted scanpath duration was 4.39 seconds (mean = 4.36s) (see figure \ref{fig:dist_hist}). The median and average similarities per dimension are stated in table \ref{table:sims}.
\begin{table}[H]
	\centering
	\begin{tabular}{lC{3cm}C{2cm}C{3cm}}
		\thickhline 
		\textbf{Variable}	& \textbf{mean} [SD] & \textbf{median}  \\ \hline
		Vector similarity 	& $0.97$ $[0.01]$   & $0.97$  \\
		Position similarity	& $0.88$ $[0.03]$   & $ 0.89$  \\			
		Length similarity	& $0.96$ $[0.01]$   & $0.96$   \\			
		Duration similarity & $0.54$ $[0.05]$   & $0.55$  \\
		Direction similarity	& $0.72$ $[0.05]$ 	& $0.71$  \\ \thickhline
		\caption{Characteristics of scanpath similarity across the movie.}
		\label{table:sims}
	\end{tabular} 
\end{table}


\begin{table}
	\begin{center}
		\begin{tabular}{C{2cm}C{2cm}C{2cm}C{2cm}C{2cm}C{2cm}}
			\thickhline
			& \textbf{Vector} & \textbf{Direction} & \textbf{Length} & \textbf{Position} & \textbf{Duration}  \\
			 \textbf{Vector} & 	- & $0.12$ & $0.97$ & $0.74$ & $-0.05$ \\ 
			 \textbf{Direction} & **  & - & $0.15$ & $0.04$ & $0.72$ \\
			 \textbf{Length} & *** & *** & - & $0.72$ & $0.04$ \\
			 \textbf{Position} & *** & n.s.	 & *** & - & $-0.12$ \\
			 \textbf{Duration } &  n.s.	& *** & n.s. & ** & - \\ \thickhline
		\end{tabular}
	\end{center}
	\footnotesize Significance codes: *** $p < 0.001$, ** $p < 0.01$, * $p < 0.05$
	\caption{Pearson correlations of similarity metrics}
	\label{table:correlations}
\end{table}
Figure \ref{fig:dist_hist} gives an overview of some additional scanpath characteristics, in particular the distribution of similarity per dimension, the onset of scanpaths in the first run, and an example of movie segments with high and low similarity.

\begin{figure}[H]
	\subfigure[Extracted scanpath lengths]{\includegraphics[width=0.49\textwidth]{img/dist_hist.png}}
	\subfigure[Distribution of similarities per dimension for the movie]{\includegraphics[width=0.49\textwidth]{img/sim_per_dimension.png}}
	\subfigure[Average onsets of scanpaths in seconds for the first run]{\includegraphics[height=0.1\textheight, width=1\textwidth]{img/scanpath_onsets.png}}
	\subfigure[Frame from movie snippet with the lowest average similarity rating of run 1, with overlaid eyegaze]{\includegraphics[height=0.18\textheight, width=0.49\textwidth]{img/low_sim.png}}
	\subfigure[Frame from movie snippet with the highest average similarity rating of run 1, with overlaid eyegaze]{\includegraphics[height=0.18\textheight, width=0.49\textwidth]{img/max_sim.png}}
	\caption{\small{Overview of extracted scanpaths' characteristics.}}
	\label{fig:dist_hist}
\end{figure}
	\textbf{TODO:learn inkscape an arrange it nicely}}

\bigskip

\section{Method validation}\label{section:results_method}
Figure \ref{fig:locsens} shows the time course of sensitivities and the corresponding GLM fit for the localizer experiment (top panel A) and the movie data (top panel B) for the first run of each of these experiments. For the localizer data, 89\% of the variance in the sensitivity time course was explained with the stimulation protocol description. For the movie data, 28\% of the variance was explained with a general location and face-occurrence annotation.
\textbf{TODO: for now, don't include classification accuracies, they are irrelevant here}
\begin{figure}
	\includegraphics[scale=0.4]{img/sens_timecourse.png}
	\caption[Functional specificity of ROIs in two validation analyses.]{\small{Black line: Sensitivities. Red line: GLM fit from regressing sensitivities onto the event regressors. A: Run 1 of the localizer experiment, with 12 regressors derived from the object categories of the stimulation. B: Run 1 of the movie experiment with regressors derived from the movie annotation.\newline Colors correspond to stimulation with one particular event type in time, beta values are given in brackets. The legend in panel B contains only the diagnostically most valuable regressors}}
	\label{fig:locsens}
\end{figure}
\section{TODO FEF}\label{section:results_FEF}
\textbf{large TODO}

\chapter{TODO Discussion}\label{section:discussion}
do a short summary of everything

\begin{itemize}
	\item The aim of this thesis was to...
	\item Therefore, in this thesis ... What did I do: MultiMatch, functional specifitity analysis
	\item The results show that...
\end{itemize}

MultiMatch\newline
The implemented multimatch algorithm was able to extract scanpaths and calculate their similarity on five different dimensions. The average length of extracted scanpaths was shorter than the duration aimed at (4.92s), as the onset and offset of the required eye movements did rarely correspond exactly to the onsets and offsets of the selected movie snippet. Nevertheless, the extracted scanpaths were similar in duration, and hence do not introduce a bias of scanpath lengths into the similarity calculation. With the exception of the movie start with three very long shots, the extracted scanpaths are distributed evenly across the movie. As evident from table \ref{table:sims} and figure \ref{fig:dist_hist}, scanpaths were almost perfectly similar on the dimensions vector length and vector position. This is likely at least partially due to the scanpath alignment based on the scanpath shape. Scanpaths were also highly similar on the position dimension, which underlines the strong gaze control of the movie stimulus. Subjects scanpaths differed more substantially on the dimensions direction and duration, which indicates differences in fixation dwelling times and saccadic angle. Thus, the general points of interest (as evident from high similarities in position, length and shape) were similar across subject, but differences in direction and duration might indicate interindividually different exploration strategies. All dimensions show a remarkable consistency in similarity measures as evident from the small standard deviations. Following the reasoning behind the implementation of the method, this might indicate a consistently high level of exogenous control by the movie stimulus. This finding is consistent with research on viewing behavior during movies: Unlike during static image viewing, the spatio-temporal gaze behavior of multiple viewers exhibits a substantial degree of coordination in movie watching. \textcite{smith2008attentional} cued the term \textit{attentional synchrony} for this phenomenon. During attentional synchrony, viewers gazes cluster around a small portion of the screen at any one moment. \textcite{goldstein2007people}, for example, found the distribution of fixations of viewers to occupy less than 12\% of the total screen area in more than 50\% of the time in six Hollywood movies. In a comparison between different types of static and dynamic visual stimuli, \textcite{dorr2010variability} found the highest consistency between viewers eyegazes during professionally produced (Hollywood) movies, likely largely due to the use of cinematic composition of scenes, deliberate camera work and editing. Hasson, Yang et al., 2008 found high correspondence in gaze behavior across subjects, even for backwards presentations of movies.

\bigskip
Functional specificity analysis \newline
The results of the validation analysis of the functional specificity analysis provide a general proof of principle for the method. It achieved a reasonable fit to the sensitivity time course, both for the localizer and the movie data (figure \ref{fig:locsens}), given the comprehensiveness of the used regressors: In the localizer paradigm, the stimulation protocol is able to explain 89\% of the variance in the sensitivity profile. As visible both from the time series' appearance and the regression coefficients, stimulation with images of faces ($\beta$ = 2.51) and, to a lesser extent, bodies ($\beta$ = 1.50), in particular the first of these stimuli per block ($\beta$=4.88 and $\beta$=2.78, respectively), differentially activated FFA more than PPA. PPA in turn was differentially activated by stimulation with scenes (scene\_first: $\beta$ = -4.96, scene = $\beta$ = -1.59). These results are in accordance to the literature (e.g. \cite{fox2009defining}, ...more here...) $\rightarrow$ proof of principle. \newline
For the movie data, the method achieved a fit of $R^2 =$ .28. The functional description of the movie was hence only able to explain less than a third of the variance in the activation differences between FFA and PPA over the course of the movie. Nevertheless, this fit was achieved with a rather coarse descriptions of a shots setting without any additional filtering, and a general face annotation, over a time span of roughly 2 hours. Moreover, the resulting regression coefficients are largely in line with previous results. The FFA is stronger activated than the PPA in the many\_faces ($\beta$=1.47) and face ($\beta$=1.04) conditions. \textbf{TODO: lets cut this here, and put the rest into future directions}


\section{TODO Drawbacks and shortcomings}
\begin{itemize}
	\item A number of potential issues need to be addressed to access the validity of the results (maybe refer to Deborah Mayos Severe Testing ideas?)
	\item As evident from the consistency in scanpath similarity, the MultiMatch method was not able to detect large changes in attentional mode throughout the movie. This might reflect a consistently high exogenous control exercised by the movie stimulus. However, it is not clear whether similarity in scanpaths does actually correspond to exogenous control. The similarity measures obtained from eye gazes could fail to account to differentiate exogenous modulation from endogenous influences arising from a shared goal between participants. Consider the following example: in order to fully comprehend the movie content, it might be of relevance for every subject to endogenously focus on the facial features of Forrest in his dialogue with Jenny. This scene does not contain salient stimulus features or other types of exogenous control, yet it would lead to a high similarity measure. Another possible confound is an endogenous attentional control evoked by the narration in the overlaid audio-description. The narrative voice describing the visual contents of the scenes could shift the viewers attention in an endogenous, yet inter-individually highly correlated fashion. A high degree of synchrony in eye movements therefore would not necessarily correspond to purely exogenous influences. A more thorough investigation of the origin of eye gaze similarity might lie in a subsequent qualitative analysis of individual movie shots, but that was not within the scope of this thesis.
	\item Problem of 5 different similarity measures --- BUT successfully used by French et al., multidimensional construct preferable as might explain more variance (citation? check anderson or the other dewhurst publication) 
	\item The thesis was largely exploratory in regard to determining the functional specificity of ROIs. Of course, the value and novelty of the developed method lies in its ability to explore previously unknown functions with naturalistic stimulation. As such, it is providing interesting new insights on the level of functional neuroanatomy, and constitutes additional evidence that the use of naturalistic stimulation is of immense value in neuroimaging research and can underlie experimental control (\cite{hasson2004intersubject}). Nevertheless, any exploratory analysis is lacking a thorough derivation of hypothesis from theory, and the preregistration of such hypotheses to guarantee scientific integrity. Furthermore, given that the choices of annotation for naturalistic stimulation and their inclusion can potentially be unlimited, the method leaves many researchers degrees of freedom to potentially bend results into shape. Therefore, the method does not suffice on its own. Instead, it should be regarded as a valuable first step to gain exploratory insights and generate novel hypotheses. The ideas derived from the method should subsequently be brought to test on new data to prevent findings due to chance being published without further testing. 
	\item Address the question of whether the linearity and gaussian distribution assumptions hold.
	\item Now trash all shortcomings and come to the conclusion that for was done and despite a number of limitations this thesis was able to advance the literature in the following ways
\end{itemize}

\section{TODO Future directions}
\begin{itemize}
	\item The employed method for determining the functional specificity of ROIs proved to be an interesting novel approach to gain further insights into the functional neuroanatomy of regions of interest. However, the achieved model fit was not optimal. A follow-up project of this thesis will explore the benefit of a different classification algorithm, a \textit{support vector machine} (SVM), and continue to explore the insights of this method in regard to a studyforrest movie dataset. Furthermore, an additional improvement to make the classification more robust could be to incorporate the voxels proximity to blood vessels (in light of the "draining vein effect"\footnote{see http://www.thebrainblog.org/2018/12/23/twenty-six-controversies-and-challenges-in-fmri/}). The studyforrest dataset might contain appropriate angiographic data from phase 1 to pursue this enhancement. 
	\item it will be especially interesting to see whether the method is capable of providing insights into functional specificities of the same structure in different hemispheres, for example the left and the right FFA.
	\item outline more promising research questions 
\end{itemize}


\section{TODO Conclusions}
\begin{itemize}
	\item now what have we all learned after reading this far?
	\item surely something needs additional research

\end{itemize}
Independent of the results, this thesis provided an open source implementation of the matlab based MultiMatch toolbox for multidimensional scanpath comparison. This might enable additional research into scanpath similarity with this geometric, multidimensional approach, without requiring usage and purchase of closed source software. \newline Furthermore, it provided a proof of principle that the sensitivities of a "transposed" approach of classification in unison with a functional description of the experimental design can be used to gain insights into the functional neuroanatomy of regions of interest in a meaningful way. The work undertaken in this thesis constitutes a starting point for further development and future application of the method.



\chapter{Things that are still shitty}
\textbf{big shit}
\begin{itemize}
	\item introduction is somewhat weak and lacks a common thread. thats because i lack a common thread though. will be easier to work on once the rest is finished.
	\item surely incomprehensible method description, especially ROI specificity analysis
	\item it's worthwhile to think about splitting the thesis from intro-methods-results-discussion into into-methods1-results1-methods2-results2-generaldiscussion
\end{itemize}
\textbf{small shit}
\begin{itemize}
	\item too many passive constructions
	\item many long, incomprehensible sentences. boil it down to something simpler
	\item I need better verbal connections between sections. Short intros and outros of all main chapters.
	\item Citations are fucked up, take care of the bibtex style, its still according to macroeconomics, there has to be an example for APA on the internet somewhere
	\item all \textbf{bold} citations are not yet in bibtex, mostly because they are weird formats.
	\item prune references extensively after recent shortening spree (maybe simply disable nocite{*} if I'm lazy?)
	\item literally just barfed every single abbreviation into the list of abbreviations, I doubt this is ultimately useful. Also still contains GLM abbreviations from vwl
	\item Things are not optimally aranged currently, once the text is in final form make sure to force figures and algorithms to their appropriate places.
	\item think of better short captions for things in the lists of figures and tables.
\end{itemize}




\pagebreak

\bigskip\bigskip

\clearpage





%------- Literaturverzeichnis -------------------
%%%% Literaturverzeichnis macht aktuell Probleme, könnte bibtex unter Debian sein
\printbibliography[	heading=bibintoc,			% oder bibnumbered
										title={Bibliography},
										nottype=online]
%									]	
\nocite{*} % auch nicht zitierte Werke ausgeben									


%--------- alternatives Literaturverzeichnis -------
% so kann man das Literaturverzeichnis direkt erzeugen
% \begin{thebibliography}{99}
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
% \bibitem{nelsen}Nelsen, Roger B.: An Introduction to Copulas, Springer, Portland, 2006. 																											
% \bibitem{lindner}	Lindner,	Alexander:	Skript zur Wahrscheinlichkeitstheorie SoSe 2014,	Braunschweig,	2014.																																											
% \end{thebibliography}


\begin{appendices}

What needs to go in here?
\begin{itemize}
	\item maybe code? though contains links to github already
	\item correlation tables? 
\end{itemize}


	
\end{appendices}




\end{document}